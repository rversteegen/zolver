# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:43:45.360727Z","iopub.execute_input":"2024-05-26T13:43:45.361561Z","iopub.status.idle":"2024-05-26T13:43:45.365845Z","shell.execute_reply.started":"2024-05-26T13:43:45.361525Z","shell.execute_reply":"2024-05-26T13:43:45.364826Z"}}
# credits:
# https://www.kaggle.com/code/abdurrafae/improved-code-interpretation
# https://www.kaggle.com/code/dnyaneshwalwadkar/submission-with-the-best-nb-new-api
# https://www.kaggle.com/code/utsavsinghal2604/natural-language-and-code-integration
# Forked from https://www.kaggle.com/code/anrenk/aimo-llm-usage-clean-code

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:59:59.223706Z","iopub.execute_input":"2024-05-26T13:59:59.224085Z","iopub.status.idle":"2024-05-26T13:59:59.228863Z","shell.execute_reply.started":"2024-05-26T13:59:59.224057Z","shell.execute_reply":"2024-05-26T13:59:59.227963Z"}}
import time

NOTEBOOK_START_TIME = time.time()
print(NOTEBOOK_START_TIME)

# %% [markdown]
# # Libraries installation

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:44:46.653039Z","iopub.execute_input":"2024-05-26T13:44:46.653404Z","iopub.status.idle":"2024-05-26T13:44:46.660171Z","shell.execute_reply.started":"2024-05-26T13:44:46.653374Z","shell.execute_reply":"2024-05-26T13:44:46.659219Z"}}
%%time
try:
    import accelerate
except:
    !pip install -U /kaggle/input/accelerate-0-29-3/accelerate-0.29.3-py3-none-any.whl -qq
    !pip install -U /kaggle/input/bitsandbytes-0-43-1/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl -qq

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:44:20.350065Z","iopub.execute_input":"2024-05-26T13:44:20.350702Z","iopub.status.idle":"2024-05-26T13:44:20.356596Z","shell.execute_reply.started":"2024-05-26T13:44:20.350663Z","shell.execute_reply":"2024-05-26T13:44:20.355626Z"}}
import os
import numpy as np
import pandas as pd
from tqdm import tqdm
import gc
import re
import sys
import subprocess
import math
import random
from collections import defaultdict
from collections import Counter
import torch
import transformers
import accelerate

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T14:36:47.288451Z","iopub.execute_input":"2024-05-26T14:36:47.288892Z","iopub.status.idle":"2024-05-26T14:36:47.295809Z","shell.execute_reply.started":"2024-05-26T14:36:47.288862Z","shell.execute_reply":"2024-05-26T14:36:47.294724Z"}}
if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):
    PRIVATE = True
else:
    PRIVATE = False

DEBUG = False
P100 = True
QUANT = False
USE_PAST_KEY = True
SEED = 313
MODEL_PATH = "/kaggle/input/deepseek-math"
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
N_REPETITIONS = 17 if PRIVATE else 2
MAX_GEN_TOKENS = 3200 #2048 if PRIVATE else 512
MAX_TOKENS = 4096 #2048 if PRIVATE else 512
    
if PRIVATE:
    NPROBS = 50
    TIME_LIMIT = 31000
else:
    NPROBS = 4
    TIME_LIMIT = 1000

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:44:20.371866Z","iopub.execute_input":"2024-05-26T13:44:20.372181Z","iopub.status.idle":"2024-05-26T13:44:20.383010Z","shell.execute_reply.started":"2024-05-26T13:44:20.372158Z","shell.execute_reply":"2024-05-26T13:44:20.382151Z"}}
if not PRIVATE:
    class train_env():
        def __init__(self, randomize=False):
            self.df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')
            self.df['ground_truth'] = self.df['answer']
            self.df['answer'] = -1

            self.df = self.df.loc[[1,4,6,10]]

            if randomize:
                self.df = self.df.reset_index().sample(frac=1).reset_index(drop=True)

            self.predict_called = True
            self.counter = 0
            self.len = len(self.df)

        def iter_test(self):
             while self.counter<self.len:
                if self.predict_called:
                    self.predict_called = False
                    test = self.df.loc[[self.counter]][['id','problem']]
                    sample_submission = self.df.loc[[self.counter]][['id','answer']]
                    yield test, sample_submission
                else:
                    print("You must call `predict()` successfully before you can continue with `iter_test()`")
                    return None  # Prevent loop

        def predict(self, answer):
            self.df.loc[self.counter, ('answer')] = answer['answer'].values[0]
            self.predict_called = True
            self.counter+=1

    make_env = train_env
    #env = train_env(randomize=True)
else:
    # Set up the evaluation API
    import aimo

    make_env = aimo.make_env
    #env = aimo.make_env()

# %% [markdown]
# # Important Custom Functions

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:44:20.384046Z","iopub.execute_input":"2024-05-26T13:44:20.384287Z","iopub.status.idle":"2024-05-26T13:44:20.397562Z","shell.execute_reply.started":"2024-05-26T13:44:20.384266Z","shell.execute_reply":"2024-05-26T13:44:20.396668Z"}}
def naive_parse(answer):
    """
    Naive parsing function to extract numerical values from a string.
    
    Args:
    - answer (str): Input string containing numerical values.
    
    Returns:
    - str: Extracted numerical values as a string.
    """
    out = []  # Initialize an empty list to store extracted numerical values
    start = False  # Flag to indicate the start of a numerical value
    end = False  # Flag to indicate the end of a numerical value
    for l in reversed(list(answer)):  # Iterate over characters in reverse order
        if l in '0123456789' and not end:  # Check if the character is a digit and not already at the end
            start = True  # Set the start flag to True
            out.append(l)  # Append the digit to the output list
        else:
            if start:  # If the start flag is True
                end = True  # Set the end flag to True
        
    out = reversed(out)  # Reverse the output list to get the original numerical value
    return ''.join(out)  # Convert the list of digits to a string and return

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:44:20.398784Z","iopub.execute_input":"2024-05-26T13:44:20.399677Z","iopub.status.idle":"2024-05-26T13:44:20.411899Z","shell.execute_reply.started":"2024-05-26T13:44:20.399645Z","shell.execute_reply":"2024-05-26T13:44:20.411032Z"}}
def output_line(output, n):
    lines = output.strip().split('\n')
    try:
        return lines[n]
    except IndexError:
        return ""

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:44:20.413300Z","iopub.execute_input":"2024-05-26T13:44:20.413586Z","iopub.status.idle":"2024-05-26T13:44:20.422977Z","shell.execute_reply.started":"2024-05-26T13:44:20.413563Z","shell.execute_reply":"2024-05-26T13:44:20.422116Z"}}
def repl(match):
    if "real" not in match.group():
        return "{}{}".format(match.group()[:-1], ', real=True)')
    else:
        return "{}{}".format(match.group()[:-1], ')')

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:44:20.424109Z","iopub.execute_input":"2024-05-26T13:44:20.424373Z","iopub.status.idle":"2024-05-26T13:44:20.434874Z","shell.execute_reply.started":"2024-05-26T13:44:20.424349Z","shell.execute_reply":"2024-05-26T13:44:20.434035Z"}}
def process_code(code):
    
    code = re.sub(r"symbols\([^)]+\)", repl, code)
    # Add a try...except block
    code = code.replace('\n', '\n    ')
    code = "\ntry:\n    from sympy import *\n{}\nexcept Exception as e:\n    print(e)\n    print('FAIL')\n".format(code)

    with open('code.py', 'w') as fout:
        fout.write(code)

    batcmd = 'timeout 7 ' + sys.executable + ' code.py'
    try:
        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')
        print("<<<<<###Result:\n" + shell_output + "\n###>>>>>")

        if output_line(shell_output, -1) == 'FAIL':
            code_status = False
            return_value = output_line(shell_output, -2)  # Last line of exception
            return_value += '\nTry checking the formatting and imports'

            if "not defined" in return_value:
                return_value+='\nTry checking the formatting and imports'
        else:
            return_value = shell_output.strip()
            code_status = True
    except Exception as e:
        print(e,'shell_output')
        return_value = -1
        code_status = False
    return return_value, code_status

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:44:20.435924Z","iopub.execute_input":"2024-05-26T13:44:20.436196Z","iopub.status.idle":"2024-05-26T13:44:20.448831Z","shell.execute_reply.started":"2024-05-26T13:44:20.436172Z","shell.execute_reply":"2024-05-26T13:44:20.448053Z"}}
def process_text_output(result):
    try:
        result_output = re.findall(r'\\boxed\{(\d+)\}', result)

        if len(result_output) == 0:
            result_output = naive_parse(result)
            print('NAIVE', result_output)
            #result_output = ""
            #return -1
        else:
            result_output = result_output[-1]
            print('BOXED', result_output)

        #print('BOXED FINAL', result_output)
        if len(result_output) == 0:
            result_output = -1
        else:
            result_output = round(float(eval(result_output))) % 1000
    except Exception as e:
        print(e)
        print('ERROR PARSING TEXT')
        result_output = -1
    
    return result_output

# %% [markdown]
# # Start of code

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T14:04:39.117963Z","iopub.execute_input":"2024-05-26T14:04:39.118828Z","iopub.status.idle":"2024-05-26T14:04:45.105894Z","shell.execute_reply.started":"2024-05-26T14:04:39.118795Z","shell.execute_reply":"2024-05-26T14:04:45.104956Z"}}
transformers.set_seed(SEED)

model_kwargs = {}

if QUANT:
    model_kwargs['quantization_config'] = transformers.BitsAndBytesConfig(
        load_in_4bit = True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

config = transformers.AutoConfig.from_pretrained(MODEL_PATH)

tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)

config.pad_token_id = tokenizer.pad_token_id

LAYERS_GPU0 = 32 if P100 else 18
device_map = [('model.embed_tokens', 0)] + [(f'model.layers.{i}', 0 if i < LAYERS_GPU0 else 1) for i in range(0, 31 + 1)] + [
                 ('model.norm', 1),
                 ('lm_head', 1)]
device_map = {ii:jj for (ii,jj) in device_map}

if P100:
    model_kwargs['device_map'] = "auto"
    #model_kwargs['device_map'] = device_map
else:
    if QUANT:
        # Fits on one device
        model_kwargs['device_map'] = "sequential"
    else:
        model_kwargs['device_map'] = device_map

if QUANT:
    quantization_config=quantization_config

model = None
torch.cuda.empty_cache()
model = transformers.AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype="auto",
    trust_remote_code=True,
    config=config,
    **model_kwargs
)

# Disable memory-efficient sparse tensors for CUDA operations
torch.backends.cuda.enable_mem_efficient_sdp(False)

print(model)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T14:44:15.602464Z","iopub.execute_input":"2024-05-26T14:44:15.602898Z","iopub.status.idle":"2024-05-26T14:44:15.614322Z","shell.execute_reply.started":"2024-05-26T14:44:15.602867Z","shell.execute_reply":"2024-05-26T14:44:15.613436Z"}}
class StoppingCriteriaSub(transformers.StoppingCriteria):
    def __init__(self, stops = [], encounters=1):
        super().__init__()
        self.stops = [stop.to("cuda") for stop in stops]

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):
        for stop in self.stops:
            suffix = input_ids[0][-len(stop):]
            if torch.all(torch.eq(stop, suffix)):
                return True
        return False

stop_words = ["```output", "```python", "```\nOutput" , ")\n```" , "```\n", "\n```\n", "``````output"] #,  
stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]
stopping_criteria = transformers.StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])

model.dtype
print(model.hf_device_map)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:58:55.727917Z","iopub.execute_input":"2024-05-26T13:58:55.728358Z","iopub.status.idle":"2024-05-26T13:58:56.011101Z","shell.execute_reply.started":"2024-05-26T13:58:55.728309Z","shell.execute_reply":"2024-05-26T13:58:56.010137Z"}}
torch.cuda.empty_cache()
gc.collect()

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:58:56.013041Z","iopub.execute_input":"2024-05-26T13:58:56.013720Z","iopub.status.idle":"2024-05-26T13:58:56.022555Z","shell.execute_reply.started":"2024-05-26T13:58:56.013684Z","shell.execute_reply":"2024-05-26T13:58:56.021672Z"}}
code = """Dear professor, consider this math problem:
\"{}\"
To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions. Your final answer should be non-negative integer, not an algebraic expression!
Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\boxed{}.

Approach:"""

code2 = """Consider this math problem:
\"{}\"
First, logically analyze the implications of the problem statement. Second, list the general steps of a Sympy-based approach to calculate the answer. Third, write out commented Sympy code to compute the numerical answer and print the result.
You can run and receive results of multiple code blocks to reach the answer in stages. 
Note that intermediate calculations may be real numbers.
Finally, output the final integer answer (not an algebraic expression) within \\boxed{{}}.
"""


cot = """Below is a math problem you are to solve:
\"{}\"
Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\boxed{{}}.\n\n"""

prompt_options = [code2, code, cot]

# Original prompts

code = """Below is a math problem you are to solve (positive numerical answer):
\"{}\"
To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. 
You can run multiple code blocks to reach the answer in steps.
Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!
Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\boxed{{}}.

Approach:"""


cot = """Below is a math problem you are to solve (positive numerical answer!):
\"{}\"
Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\boxed{{}}.\n\n"""


code2 = """Consider this math problem:
\"{}\"
First, analyze the implications of the problem statement and restate it more mathematically. Write code to check assumptions, to simplify the problem.
Write out commented Sympy code to compute the numerical answer and print the result.
Think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\boxed{{}}.\n\n"""


prompt_options = [code,cot, code2]

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T14:24:14.550146Z","iopub.execute_input":"2024-05-26T14:24:14.550652Z","iopub.status.idle":"2024-05-26T14:24:14.581349Z","shell.execute_reply.started":"2024-05-26T14:24:14.550620Z","shell.execute_reply":"2024-05-26T14:24:14.580467Z"}}

class LLMGenerator:
    def __init__(self):
        self.prompt = ""
        self.num_gen_tokens = 0  # Not including prompt and outputs
        self.past_key_values = None
        self.stop = False

    def initial_prompt(self, text, assist_prefix = ""):
        if True:
            self.prompt = "User: " + text
        else:
            self.messages = [{"role": "user", "content": text}]
            if assist_prefix:
                self.messages.append({"role": "assistant", "content": assist_prefix})
        self.update_inputs()
        print(f"<<<<<PROMPT {self.input_len} tokens\n" + text + "\n>>>>>")

    def update_inputs(self):
        if True:
            self.model_inputs = tokenizer(self.prompt, return_tensors='pt').to(model.device)
        else:
            self.model_inputs = tokenizer.apply_chat_template(self.messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
        self.input_len = len(self.model_inputs['input_ids'][0])

    def append_prompt(self, text):
        self.prompt += text
        old_input = self.input_len
        self.update_inputs()
        print(f"<<<<<APPEND {self.input_len - old_input} tokens\n" + text + "\n>>>>>")

    def generate(self, temperature = 0.9, top_p = 1.0):
        startt = time.time()

        max_toks = min(MAX_TOKENS - self.input_len, MAX_GEN_TOKENS - self.num_gen_tokens)
        generation_output = model.generate(**self.model_inputs, 
                                           max_new_tokens = max_toks,
                                           return_dict_in_generate = USE_PAST_KEY,
                                           past_key_values = self.past_key_values,
                                           do_sample = True,
                                           temperature = temperature,
                                           top_p = top_p,
                                           pad_token_id = tokenizer.eos_token_id,
                                           num_return_sequences = 1,
                                           stopping_criteria = stopping_criteria)

        if USE_PAST_KEY:
            self.tokens = generation_output.sequences[0]
            self.past_key_values = generation_output.past_key_values
        else:
            self.tokens = generation_output[0]
        self.decoded_output = tokenizer.decode(self.tokens, skip_special_tokens = False) #True)
        self.new_output = self.decoded_output[len(prompt):]
        #self.new_output = tokenizer.decode(gen.tokens[self.input_len:], skip_special_tokens=True)

        runt = time.time() - startt

        new_toks = len(self.tokens) - self.input_len
        self.num_gen_tokens += new_toks
        if self.num_gen_tokens >= MAX_GEN_TOKENS:
            print("HIT MAX_GEN_TOKENS")
            self.stop = True
        if len(self.tokens) >= MAX_TOKENS:
            print("HIT MAX_TOKENS")
            self.stop = True

        print(f"<<<<<GEN {new_toks} tokens ({len(self.tokens)} total) in {runt :.1f}s ({new_toks/runt :.1f} tok/s)\n"
              + self.new_output
              + "\n>>>>>")

    def endswith(self, text):
        return self.decoded_output.endswith(text)


def predict(probi, problem):

    temperature = 0.9
    top_p = 1.0
    temperature_coding = 0.9
    top_p_coding = 1.0

    score = 0
    best = 0
    outputs = []  # List of (answer, score, info) tuples
    answer_scores = defaultdict(int)  # answer -> total_score

    time_left = TIME_LIMIT - (time.time() - NOTEBOOK_START_TIME)
    time_for_item = time_left / max(1, NPROBS - probi)
    item_time_start = time.time()
    for jj in tqdm(range(N_REPETITIONS)):
        time_spent = time.time() - NOTEBOOK_START_TIME
        spent_this_prob = (time.time() - item_time_start)
        print(f"\n\n----QUESTION {probi} - rep.{jj} - time_spent : {time_spent:.0f}/{TIME_LIMIT}, on this prob: {spent_this_prob}/{time_for_item:.0f} secs")
        if time_spent > TIME_LIMIT or spent_this_prob > time_for_item:
            break
        
        for _ in range(5):
            torch.cuda.empty_cache()
            gc.collect()
            time.sleep(0.2)
        
        last_code_error = None
        code_error_count = 0
        result_info = "NA"
        code_output = "-1"
        cumulative_code = ""

        try:
            gen = LLMGenerator()

            prompt = prompt_options[jj % len(prompt_options)]
            prompt = prompt.format(problem)

            gen.initial_prompt(prompt)
            gen.generate(temperature, top_p)

            while not gen.stop:
                if not any(gen.endswith(stop_word) for stop_word in stop_words):
                    break

                if gen.endswith("```python"):
                    temperature_inner = temperature_coding
                    top_p_inner = top_p_coding
                else:
                    temperature_inner = temperature
                    top_p_inner = top_p

                    code_status = False
                    try:
                        if gen.endswith("``````output"):
                            print("(((Weird ``````output)))")
                            code_text = decoded_output.split('```python')[-1].split("``````")[0]
                        else:
                            code_text = decoded_output.split('```python')[-1].split("```")[0]

                        all_code = cumulative_code + code_text
                        code_output, code_status = process_code(all_code)
                        #print('<<<<<CODE RESULTS\n' + code_output + ">>>>>")

                        if code_status == True:
                            code_error_count = 0
                            cumulative_code += code_text
                        else:
                            # code_output is the exception line
                            if code_output == last_code_error:
                                code_error_count += 1
                            else:
                                code_error_count = 1
                            last_code_error = code_output
                            if code_error_count >= 2:
                                print("REPEATED ERROR")
                                break

                    except Exception as e:
                        print(e)
                        print('ERROR PARSING CODE')
                        code_output = ""

                    #if gen.endswith(")\n```"):
                    if gen.endswith("\n```"):
                        if not gen.endswith(")\n```"):
                            print("(((doesn't end with )\\n```!)))")
                        gen.append_prompt('```output\n' + code_output + '\n```\n')
                    else:
                        print("(((doesn't end with \\n```)))")
                        #gen.append_prompt('\n' + code_output + '\n```\n')
                        gen.append_prompt('\n>>> ' + code_output + '\n\n')

                    # if code_status:
                    #     #if gen.endswith(")\n```"):
                    #     if gen.endswith("\n```"):
                    #         gen.append_prompt('```output\n' + code_output + '\n```\n')
                    #     else:
                    #         #gen.append_prompt('\n' + code_output + '\n```\n')
                    #         gen.append_prompt('\n>>> ' + code_output + '\n\n')
                    # else:
                    #     pass #cumulative_code = ""

                    gen.generate(temperature_inner, top_p_inner)

            result_output = process_text_output(gen.new_output)
            if result_output == -1:
                # Fallback
                try:
                    result_output = round(float(eval(code_output.strip().split("\n")[-1]))) % 1000
                    print("code_output fallback got:", result_output)
                    result_info = 
                    score = 0.6
                except Exception as e:
                    print(e, 'code_output fallback parse failed')
            else:
                score = 1

        except Exception as e:
            print("predict() EXCEPTION")
            print(e)
            result_output = -1

        if result_output != -1:
            outputs.append((result_output, score, result_info))
            answer_scores[result_output] += score

        if len(outputs) > 0:
            answers = [(score,ans) for (ans,score) in answer_scores.items()]
            answers.sort(reverse = True)
            print("SCORES,ANSWERS:", answers)
            best_score, best = answers[0]
            #if best_score >= 3 and best_score >= 1 + (jj+1)/2:
            if best_score > 5:
                print("ANSWER FOUND!")
                break

    print("\nAll outputs:", outputs)
    return best

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T14:24:36.933262Z","iopub.execute_input":"2024-05-26T14:24:36.933623Z","iopub.status.idle":"2024-05-26T14:29:56.119910Z","shell.execute_reply.started":"2024-05-26T14:24:36.933596Z","shell.execute_reply":"2024-05-26T14:29:56.118255Z"}}
env = make_env()
iter_test = env.iter_test()

NOTEBOOK_START_TIME = time.time()
for probi, (test, sample_submission) in enumerate(iter_test):
    sample_submission['answer'] = predict(probi, test['problem'].values[0])
    #print(f"Making prediction for ""{test[:100]}"": {sample_submission}")
    env.predict(sample_submission)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-26T13:58:56.056359Z","iopub.status.idle":"2024-05-26T13:58:56.056798Z","shell.execute_reply.started":"2024-05-26T13:58:56.056613Z","shell.execute_reply":"2024-05-26T13:58:56.056633Z"}}
if not PRIVATE:
    print(env.df)
    score = (env.df.ground_truth == df.model_answer).sum()
    print(f'{score} matches in {len(env.df)} examples')

# %% [code] {"jupyter":{"outputs_hidden":false}}
